This describes only the yumyum-specific functionality present in this element.

When using yumyum functionality, the rest of the Scheduler functionality will
remain unaffected.  Yumyum-specific functionality simply augments the
capabilities of the Scheduler to add the ability to simulate fault and failure
modes into SST simulations of systems, as well as a hierarchy of nodes that
allows for cascading faults to affect the ability of the simulated system to
function optimally. This functionality is optional, and will not affect non-
yumyum simulations.

---
node hierarcy
---

To create a node hierarchy, simply create two nodeComponents, joined with a
link with the parent nodeComponent end of the link using the "Child[n]" port,
and the child nodeComponent end of the link using the "Parent[n]" port, where
[n] is a number that iterates from zero to n, where n is the number of links
connected to that node of a single given type.

---
faults
---

Faults are simulated by creating a "faultActivationRate" element under the
params for a nodeComponent.  This will contain CSV describing the faults and
the fault rates:

<faultActivationRate>
  "fault type 1", "1337",
  "fault type 2", "42"
</faultActivationRate>

Where the fault rate (called lambda) occurs at a rate described in
nodeComponent.cc in

void nodeComponent::sendNextFault( std::string faultType )

These faults can be logged by Scheduler in two files:

- fault.log - ground truth, this describes what faults have occured.
  Filename is described using the faultLogFileName param.

- error.log - logs faults according the probabilities described using the
  the errorMessageProbability param:

  <params>
    <errorMessageProbability>
      "fault type 1", "0.5",
      "fault type 2", "0.95"
    </errorMessageProbability>
    ...
  </params>

  Filename is described using the errorLogFilename param

These faults will travel down the hierarchy of nodes, from parent to child
until they reach nodes connected to the schedComponent.  By default, the
nodes will end whatever job they are running by whenever they recieve a
fault, or according to some probability that can be defined using the
jobFailureProbability param, in a similar fashion to errorMessageProbability.

  <jobFailureProbability>
    "fault type 1", "0.5"
  </jobFailureProbability>

This jobFailureProbability parameter can be placed on any nodeComponent.  When
an error travels through the node, the jobFailureProbability associated with
that fault type is examined to determine if the job on the scheduled node the
error will eventually arrive at should be killed.  If yes, then the any
scheduled node the error eventually arrives at will kill the job.  If no, then
the error is sent on normally.  This jobFailureProbability check will be done
at every node that has the jobFailureProbability parameter.  Effectively, a job
will be killed if any node between, and including, the initial node which
causes the fault and the schedulable node which receives the error, has a
jobFailureProbability parameter and passes the probability check.

---
fault propagation delays
---

The time taken by a fault to travel from node A to node B is by default zero.
This can be adjusted using the errorPropagationDelay parameter on a
nodeComponent:

  <errorPropagationDelay>
    "retransmit", "0", "2",
    "thermal", "1", "1",
    "lost connection", "2", 5"
  </errorPropagationDelay>

The columns in the dalay parameter are the fault name, min delay time, max
delay time. If a fault has no specified time, it will always be transmitted
to all children of a given node on the same timestep it is recieved, instantly
propagating through the graph.  Otherwise, a uniformly-distributed random number
is selected using the two specified numbers as bounds.  (inclusive)  The node
then waits that length of time before transmitting the fault to its children.

For each fault traveling through the graph, the delay is calculated
independently for each link it traverses.

---
yumyum jobs
---

The original Scheduler trace format can still be used to create simulated jobs,
but there is also an optional yumyum format that may be used instead.  This
format uses three columns to describe jobs to be run:

jobID, seconds needed, nodes needed.

Thus a job "12, 1337, 42" indicates that job 12 needs 1337 seconds on 42 nodes.

If this format is to be used, add the following param to schedComponent:

<useYumYumTraceFormat>true</useYumYumTraceFormat>

---
yumyum logging
---

The scheduler can log the jobs run, with the result (finish/fault).  To use
this, add the following to the schedComponent's params:

<jobLogFileName>job.log</jobLogFileName>
<printYumYumJobLog>true</printYumYumJobLog>
<printJobLog>true</printJobLog>

Where job.log is the deired file to print the log.  The log will have the
following format:

[job ID], [start time], [end time], [pass/fault], [nodes used]

If the end time and pass/fault value are -1, then the log entry is for the job
start.  If the pass/fault value is 0, then the log entry is for a job
finishing.  If the pass/fault value is 1, the a fault prevented the job from
finishing, and the job end time is the second of the simulation in which a
fault was simulated.

"1","0","-1","-1","1.4 1.3 1.2 1.1"             # job start
"1","0","100000","0","1.4 1.3 1.2 1.1"          # job finish
"2","100000","-1","-1","1.1 1.2"                # job start
"2","100000","130833","1","1.1 1.2"             # a fault stopped the job

---
simulation ending
---

Normally, the simulation will end as soon as the last job finished being
simulated.  This behavior can be adjusted using the schedComponent param:

<useYumYumSimulationKill>true</useYumYumSimulationKill>

If this param is used, then the simulation will not end when the last job is
finished, but the scheduler will block while waiting for the job trace to be
modified.  When the job log is modified, all new jobs are read out of it, and
these are simulated.  This behavior will contimue.  The only way to end the
simulation will be to either kill SST, or write "YYKILL" to the job list. This
will cause the schedComponent to unregister itself from SST, and the
simulation will end.

While waiting for the job trace to be written to, schedComponent will poll the
trace file at a frequency defined by

<YumYumPollWait>n</YumYumPollWait>

where n is the number of ms for which schedComponent should sleep between
checking the file for a new modification time.

---
yumyum PRNG
---

The seed used for the PRNG used for the yumyum functionality discussed here can
be specified as a param to the schedComponent in the XML, like so:

  <params>
    <seed>42</seed>
  </params>

If a seed is not specified, the PRNG will be seeded with the current time, with
a one second resolution.

This seed is used only for yumyum functionality; all other SST functionality
remains the same, as does functionality in other elements.

---
yumyum functionality tests
---

A set of tests have been created which can be used to verify that the yumyum
functionality is working correctly.  These tests can be found in the yumyum
subdirectory, and can be run (after building SST and Scheduler, of course)
using the runtests.pl script:

perl runtests.pl

This will run a series of simulations that will test nodeComponent and
schedComponent, including yumyum functionality.  A few short simulations are
run to test basic functionality, and many longer tests are run to test that the
simulated results are statistically believable, as well as to test for
regressions in code correctness and performance.

---
yumyum functionality example
---

An example of a small graph with a number of faults and errors can be found
under yumyum/xml/example.xml.

