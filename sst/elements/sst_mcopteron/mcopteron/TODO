
TODO

-- MAKE Simulator microbenchmark traces!!!
-- are use distance histograms being put on all intructions?
-- having an output dependency go to multiple instructions??

0. Need to add input file reading, and allow parameterization with command-line
arguments. Essentially, we want to make running a workload under different scenarios 
easy. It should be easy to tweak the hardware components (e.g., queue sizes, # of
functional units), memory latencies (e.g., simulate with 0 memory cost to see 
"optimal" architecture performance)

Issues:

1. Reorder buffer might have some things to fix
  a. the check for mispredicted branches looks like it is happening before
     the branch itself is committed; this needs verified and fixed, if true.
  b. the fake LEAs for FP memops are in a fake buffer, but this is not 
     coordinated with when canceling instructions due to a mispredict; should be.
  c. question: do instructions _really_ occupy a reservation queue slot until
     retirement? or if they complete do they stay in the reorder buffer but 
     get cleaned out of the reservation queue? do instructions really get a slot
     both in the reorder buffer and a reservation station immediately upon 
     dispatch? That's the way the simulator is doing it now.
     
Older open and closed issues

1. Memory modeling. 

DONE: We are not modeling instruction fetch or load/store costs yet. 
DONE: Especially we are not modeling load-to-use delays -- right now an instruction
      satisfies all dependencies when its inherent latency is completed, but this 
      is wrong for loads; they should take longer, depending on how fetched. 

We only use the general instruction-to-use histogram for both reg-to-use and 
mem-to-use dependencies.

Our memory model uses a st-ld forward probability, the LS queue does nothing
in these regards; for the Opteron the forward delay is exactly the same as 
the L1 cache delay anyways (two cycles), so this is not a big deal unless we
want to simulate a change here.
  
Instruction fetch may need more modeling; we are fetching at most three 
instructions per cycle, and if mem says to stall we do; but double dispatch
insns need to be handled, they should occupy two of the three slots, and
vector path should occupy WHO KNOWS HOW MANY!). double dispatch insns maybe
should occupy two slots everywhere: reservation queue, reorder buffer, etc.
This would also affect statistics like average queue occupancy.
    
2. Some instruction definitions need fixed and regularized.

Much progress here, though there are probably still some that need fixed.
  
DONE: many of the separate insn defs for those with memory ops should simply be
      removed, because they encode the latency that we are simulating already in
      the memory model.

We are assuming AGU operands are immediately ready; this obviously is not
always true but most probably the operand dependencies are for data operands,
not AGU operands. We would need a separate AGU operand distance table if we
are to simulate this. That might be HARD in pin.

2a. Need to read operand column of instruction and set some flags to indicate
    operand usage and direction
    
DONE: we are picking up instruction size and direction (I think) 
  - still need lots of testing here, there may be some more work here, plus
    we removed many of the mem-dirs so that we are now using reg-reg insn for
    many that have memory operands.

3. Complex execution sequences. Some instructions seem to have sequential
functional unit use, but we are not modeling that yet. We only model address
generation (if needed) and then "regular" functional unit execution. The
instruction information has the beginnings of a sequence model in that 
the field execUnitMask is meant to be a four-step bitmask, where each byte
is a mask with bits set for which execution units can be used for that 
step. But the simulation is only setting and using the first byte right now.
One question is: should we allow address generation to be the first byte, or
continue deciding it on the fly?

4. Lots more statistics. (partially done)

5. Pintool needs to generate load-store distance histogram? Do we care?

From chip-architect.com:

- "integer pipeline handles all loads and stores, including for FP/MMX instructions.
  - this means that somehow an AGU is used on FP insns that access memory
    
DONE: we generate a fake LEA instruction if we get an FP insn with a memop;
  it is then sent to an INT queue; it is marked as fake so is not counted in the
  total # of instructions, although each queue counts it as an insn that is 
  processed; it does not occupy a slot in the reorder buffer or anywhere else other
  than the queue it goes to; a dependency is set up between it and the actual FP
  instruction, but currently the FP insn just waits until 1 dependency is registered
  as ready to flag the load-store unit that the address is ready; some other insn
  could set that flag before the fake-LEA does, so this is somewhat inaccurate.
  
- Are we measuring condition code use distance in the pintool? We should, and
  we should use it on conditional branch instructions (how?)
- Instruction fetch takes two cycles to decode insns to place into queues
  - we ignore this because we assume it is essentially masked into pipelining
  - but maybe on a mispredict it matters
- L1 data cache takes two cycles, plus one for address generation.
  - had picture saying 
    - cycle 0, place in i-queue and ls1-queue
    - cycle 1, schedule on agu 
    - cycle 2, agu gens address, load scheduled on ls1
    - cycle 3, cache address decode
    - cycle 4, cache data access, alu scheduled
    - cycle 5, alu op performed
  - is cycle 1 really needed? Does it need simulated? Is it in the tables?
  - ANSWER: we aren't really simulating the scheduling step (cycle 1) because it
    is performed by the queue seeing what is coming one cycle ahead of time, and 
    thus is somewhat invisible; we just send an insn to a FuncUnit in the cycle
    we see the operands are ready; the only case this misses a cycle is when an
    instruction first reaches a queue and is then immediately sent to a FuncUnit;
    this should be rare (?) other than at startup...
    (I suppose we might be able to increase all latencies by one, but is this right?)
    
- Some instructions can be load AND store. Are we allowing/handling this?
  - for now, NO; this is important for, e.g., push/pop to/from memory, and for indirect
    calls (which read mem and push return address).
  
- direct path decode: 1 insn == 1 macro op
- vector path decode: 1 insn == 2+ macro ops
  -- it sounds like these take an arbitrary number of the 3 decode slots per
     cycle, even taking more than one cycle. I think we should just ignore them
     as they probably don't occur very often
- double dispatch decode 1 insn == exactly 2 macro ops
  -- e.g., some 128-bit FP ops are split into 2 64-bit ops
  -- it sounds like these really do take 2 of the 3 dispatch slots; in that case
     we should only allow one more insn that cycle, OR if it is in the last slot
     we need to split it and take the one plus one in the next cycle
- queue scheduler will peek at L1 results, and send an ALU op to
  the ALU even if a miss; if miss happens then op is kicked out of
  ALU; (this avoid a 1-cycle delay supposedly?), but it should occupy the
  ALU for one cycle
  - I hope we don't decide to model this!

